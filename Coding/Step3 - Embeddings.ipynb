{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center> Step 3 - Embedding </center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 102
    },
    "colab_type": "code",
    "id": "cNzM62k6RAHj",
    "outputId": "e42593b0-84d5-4f1f-e068-5c1baf344b66"
   },
   "outputs": [],
   "source": [
    "#import modules\n",
    "\n",
    "import os.path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "\n",
    "import re\n",
    "import gensim\n",
    "from gensim import corpora, similarities\n",
    "from gensim.test.utils import common_dictionary, common_corpus, get_tmpfile\n",
    "from gensim.models import LsiModel, FastText\n",
    "from gensim.models.coherencemodel import CoherenceModel\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 71,
     "resources": {
      "http://localhost:8080/nbextensions/google.colab/files.js": {
       "data": "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",
       "headers": [
        [
         "content-type",
         "application/javascript"
        ]
       ],
       "ok": true,
       "status": 200,
       "status_text": ""
      }
     }
    },
    "colab_type": "code",
    "id": "A6J43sleVCN4",
    "outputId": "506653ac-1abf-4ce6-cd12-151620b10f70"
   },
   "outputs": [],
   "source": [
    "# for use on google colab\n",
    "from google.colab import files\n",
    "uploaded = files.upload()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "iCe_EKmZVTIt"
   },
   "outputs": [],
   "source": [
    "def read_jl_file(file_name):\n",
    "    values = []\n",
    "    with open(file_name, 'rb') as f:\n",
    "        line = '---'\n",
    "        while len(line)>1:\n",
    "            line = f.readline()\n",
    "            values.append(line)\n",
    "    values = values[:-1]\n",
    "    values = [json.loads(i) for i in values]\n",
    "    df = pd.DataFrame(values)\n",
    "    return df\n",
    "\n",
    "# Reading file\n",
    "#df = read_jl_file('/Users/Thomas/Documents/Data Science X/Cours/Capgemini project/1 - Scrapping : SWOT/ReviewRestoSpider.jl')\n",
    "\n",
    "df = read_jl_file('/Users/Thomas/Documents/GitHub/Capgemini_NLP_project-ongoing-/Coding/ReviewRestoSpider_10pages.jl')\n",
    "df.Rating = df.Rating.apply(lambda x: x[0])\n",
    "df.Rating = df.Rating.apply(lambda x: int(x[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 419
    },
    "colab_type": "code",
    "id": "iX_zUHvGVt_K",
    "outputId": "0fff6bdd-5ef5-49fe-9935-aa6f0be11fa3"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>partial content</th>\n",
       "      <th>name</th>\n",
       "      <th>title</th>\n",
       "      <th>Restaurant_name</th>\n",
       "      <th>Rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>I have been using this restaurant for years. T...</td>\n",
       "      <td>tracey h</td>\n",
       "      <td>Best authentic Food, Chefs are top quality</td>\n",
       "      <td>[ Karahi Junction ]</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>The most amazing breakfast, ingredients fresh ...</td>\n",
       "      <td>kazowen</td>\n",
       "      <td>Breakfast Bliss</td>\n",
       "      <td>[ Melucci's ]</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>We went with friends ( 2 couples). The food he...</td>\n",
       "      <td>elthamfams</td>\n",
       "      <td>Excellent Food</td>\n",
       "      <td>[ Melucci's ]</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>So why not five stars? Everything was was very...</td>\n",
       "      <td>richardcC3581NV</td>\n",
       "      <td>Friendly staff, good food, good portions</td>\n",
       "      <td>[ Awesome Thai ]</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>Great food, friendly staff, decent prices. I h...</td>\n",
       "      <td>leona939</td>\n",
       "      <td>Great food</td>\n",
       "      <td>[ Namaste Gurkha ]</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3648</td>\n",
       "      <td>We were a large group from the states; was the...</td>\n",
       "      <td>I8025IKdonnal</td>\n",
       "      <td>Over the top exceptional!</td>\n",
       "      <td>[ Mezzet Lebanese Restaurant ]</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3649</td>\n",
       "      <td>Mezzet is always very welcoming and you will n...</td>\n",
       "      <td>VMD67</td>\n",
       "      <td>Simply Delicious</td>\n",
       "      <td>[ Mezzet Lebanese Restaurant ]</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3650</td>\n",
       "      <td>Visited for our friends anniversary lunch. We ...</td>\n",
       "      <td>GarethrT</td>\n",
       "      <td>Anniversary lunch</td>\n",
       "      <td>[ The French Table ]</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3651</td>\n",
       "      <td>We recently went to Mezzet for the second time...</td>\n",
       "      <td>jackdL849SV</td>\n",
       "      <td>Fantastic Meal!</td>\n",
       "      <td>[ Mezzet Lebanese Restaurant ]</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3652</td>\n",
       "      <td>Came here as a group of six. Good value and th...</td>\n",
       "      <td>robinmB207OY</td>\n",
       "      <td>Best in the area</td>\n",
       "      <td>[ Mezzet Lebanese Restaurant ]</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3653 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        partial content             name  \\\n",
       "0     I have been using this restaurant for years. T...         tracey h   \n",
       "1     The most amazing breakfast, ingredients fresh ...          kazowen   \n",
       "2     We went with friends ( 2 couples). The food he...       elthamfams   \n",
       "3     So why not five stars? Everything was was very...  richardcC3581NV   \n",
       "4     Great food, friendly staff, decent prices. I h...         leona939   \n",
       "...                                                 ...              ...   \n",
       "3648  We were a large group from the states; was the...    I8025IKdonnal   \n",
       "3649  Mezzet is always very welcoming and you will n...            VMD67   \n",
       "3650  Visited for our friends anniversary lunch. We ...         GarethrT   \n",
       "3651  We recently went to Mezzet for the second time...      jackdL849SV   \n",
       "3652  Came here as a group of six. Good value and th...     robinmB207OY   \n",
       "\n",
       "                                           title  \\\n",
       "0     Best authentic Food, Chefs are top quality   \n",
       "1                                Breakfast Bliss   \n",
       "2                                 Excellent Food   \n",
       "3       Friendly staff, good food, good portions   \n",
       "4                                     Great food   \n",
       "...                                          ...   \n",
       "3648                   Over the top exceptional!   \n",
       "3649                            Simply Delicious   \n",
       "3650                           Anniversary lunch   \n",
       "3651                             Fantastic Meal!   \n",
       "3652                            Best in the area   \n",
       "\n",
       "                     Restaurant_name  Rating  \n",
       "0                [ Karahi Junction ]       5  \n",
       "1                      [ Melucci's ]       5  \n",
       "2                      [ Melucci's ]       5  \n",
       "3                   [ Awesome Thai ]       4  \n",
       "4                 [ Namaste Gurkha ]       5  \n",
       "...                              ...     ...  \n",
       "3648  [ Mezzet Lebanese Restaurant ]       5  \n",
       "3649  [ Mezzet Lebanese Restaurant ]       5  \n",
       "3650            [ The French Table ]       5  \n",
       "3651  [ Mezzet Lebanese Restaurant ]       5  \n",
       "3652  [ Mezzet Lebanese Restaurant ]       5  \n",
       "\n",
       "[3653 rows x 5 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "7iRNpbdTdgBA",
    "outputId": "3742dbbc-64ea-41d7-b462-ae54c1b41b8a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(df['partial content'][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing (data cleaning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 289
    },
    "colab_type": "code",
    "id": "0rRU0ZlbVxj_",
    "outputId": "9f458137-ecea-44f5-f4da-31a3595dfcad"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>partial content</th>\n",
       "      <th>name</th>\n",
       "      <th>title</th>\n",
       "      <th>Restaurant_name</th>\n",
       "      <th>Rating</th>\n",
       "      <th>cleaned_description</th>\n",
       "      <th>clean_description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>I have been using this restaurant for years. T...</td>\n",
       "      <td>tracey h</td>\n",
       "      <td>Best authentic Food, Chefs are top quality</td>\n",
       "      <td>[ Karahi Junction ]</td>\n",
       "      <td>5</td>\n",
       "      <td></td>\n",
       "      <td>[use, restaur, year, chef, excel, alway, good,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>The most amazing breakfast, ingredients fresh ...</td>\n",
       "      <td>kazowen</td>\n",
       "      <td>Breakfast Bliss</td>\n",
       "      <td>[ Melucci's ]</td>\n",
       "      <td>5</td>\n",
       "      <td></td>\n",
       "      <td>[amaz, breakfast, ingredi, fresh, cook, perfec...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>We went with friends ( 2 couples). The food he...</td>\n",
       "      <td>elthamfams</td>\n",
       "      <td>Excellent Food</td>\n",
       "      <td>[ Melucci's ]</td>\n",
       "      <td>5</td>\n",
       "      <td></td>\n",
       "      <td>[went, friend, 2, coupl, food, excel, staff, g...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>So why not five stars? Everything was was very...</td>\n",
       "      <td>richardcC3581NV</td>\n",
       "      <td>Friendly staff, good food, good portions</td>\n",
       "      <td>[ Awesome Thai ]</td>\n",
       "      <td>4</td>\n",
       "      <td></td>\n",
       "      <td>[five, star, everyth, pleasant, busi, popular,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>Great food, friendly staff, decent prices. I h...</td>\n",
       "      <td>leona939</td>\n",
       "      <td>Great food</td>\n",
       "      <td>[ Namaste Gurkha ]</td>\n",
       "      <td>5</td>\n",
       "      <td></td>\n",
       "      <td>[great, food, friendli, staff, decent, price, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     partial content             name  \\\n",
       "0  I have been using this restaurant for years. T...         tracey h   \n",
       "1  The most amazing breakfast, ingredients fresh ...          kazowen   \n",
       "2  We went with friends ( 2 couples). The food he...       elthamfams   \n",
       "3  So why not five stars? Everything was was very...  richardcC3581NV   \n",
       "4  Great food, friendly staff, decent prices. I h...         leona939   \n",
       "\n",
       "                                        title      Restaurant_name  Rating  \\\n",
       "0  Best authentic Food, Chefs are top quality  [ Karahi Junction ]       5   \n",
       "1                             Breakfast Bliss        [ Melucci's ]       5   \n",
       "2                              Excellent Food        [ Melucci's ]       5   \n",
       "3    Friendly staff, good food, good portions     [ Awesome Thai ]       4   \n",
       "4                                  Great food   [ Namaste Gurkha ]       5   \n",
       "\n",
       "  cleaned_description                                  clean_description  \n",
       "0                      [use, restaur, year, chef, excel, alway, good,...  \n",
       "1                      [amaz, breakfast, ingredi, fresh, cook, perfec...  \n",
       "2                      [went, friend, 2, coupl, food, excel, staff, g...  \n",
       "3                      [five, star, everyth, pleasant, busi, popular,...  \n",
       "4                      [great, food, friendli, staff, decent, price, ...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def preprocess_data(raw_text):\n",
    "    \"\"\"\n",
    "    Input  : raw text to clean\n",
    "    Purpose: preprocess (i.e. clean) text (tokenizing, removing stopwords, stemming)\n",
    "    Output : preprocessed text\n",
    "    \"\"\"\n",
    "    # initialize regex tokenizer\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    \n",
    "    # create English stop words list\n",
    "    en_stop = set(stopwords.words('english'))\n",
    "    \n",
    "    # Create p_stemmer of class PorterStemmer\n",
    "    p_stemmer = PorterStemmer()\n",
    "    \n",
    "    # initialize list for tokenized documents in loop\n",
    "    df[\"cleaned_description\"] = \"\"\n",
    "    \n",
    "    # clean and tokenize document string\n",
    "    if raw_text!=None:\n",
    "        raw = raw_text.lower()\n",
    "    else:\n",
    "        raw = \"none\"\n",
    "    tokens = tokenizer.tokenize(raw)\n",
    "    \n",
    "    # remove stop words from tokens\n",
    "    stopped_tokens = [i for i in tokens if not i in en_stop]\n",
    "    \n",
    "    # stem tokens\n",
    "    stemmed_tokens = [p_stemmer.stem(i) for i in stopped_tokens]\n",
    "\n",
    "    return stemmed_tokens\n",
    "\n",
    "# clean\n",
    "df[\"clean_description\"] = df[\"partial content\"].apply(preprocess_data)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dictionary(doc_clean):\n",
    "    \"\"\"\n",
    "    Input  : clean document\n",
    "    Purpose: get the whole associated vocabulary\n",
    "    Output : term dictionary where every unique term is assigned an index\n",
    "    \"\"\"\n",
    "    return corpora.Dictionary(doc_clean)\n",
    "\n",
    "# get vocabulary\n",
    "dictionary = get_dictionary(df.clean_description)\n",
    "for i,j in dictionary.items():\n",
    "    print(\"word =\", j, \"--> ID =\", i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TFIDF (built from scratch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 439
    },
    "colab_type": "code",
    "id": "v_SzTJ83XLUJ",
    "outputId": "4abc0338-b503-4571-b021-e2dc4f887a44"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>3643</th>\n",
       "      <th>3644</th>\n",
       "      <th>3645</th>\n",
       "      <th>3646</th>\n",
       "      <th>3647</th>\n",
       "      <th>3648</th>\n",
       "      <th>3649</th>\n",
       "      <th>3650</th>\n",
       "      <th>3651</th>\n",
       "      <th>3652</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.396297</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.475461</td>\n",
       "      <td>0.190184</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.090799</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.215121</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.156819</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.190390</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6265</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6266</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0256</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6267</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0256</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6268</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0256</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6269</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0435</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6270 rows × 3653 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          0     1     2     3     4     5     6     7     8     9     ...  \\\n",
       "0     0.396297   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  ...   \n",
       "1     0.090799   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  ...   \n",
       "2     0.215121   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  ...   \n",
       "3     0.156819   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  ...   \n",
       "4     0.190390   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  ...   \n",
       "...        ...   ...   ...   ...   ...   ...   ...   ...   ...   ...  ...   \n",
       "6265  0.000000   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  ...   \n",
       "6266  0.000000   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  ...   \n",
       "6267  0.000000   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  ...   \n",
       "6268  0.000000   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  ...   \n",
       "6269  0.000000   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  ...   \n",
       "\n",
       "      3643  3644  3645  3646  3647    3648      3649      3650  3651    3652  \n",
       "0      0.0   0.0   0.0   0.0   0.0  0.0000  0.475461  0.190184   0.0  0.0000  \n",
       "1      0.0   0.0   0.0   0.0   0.0  0.0000  0.000000  0.000000   0.0  0.0000  \n",
       "2      0.0   0.0   0.0   0.0   0.0  0.0000  0.000000  0.000000   0.0  0.0000  \n",
       "3      0.0   0.0   0.0   0.0   0.0  0.0000  0.000000  0.000000   0.0  0.0000  \n",
       "4      0.0   0.0   0.0   0.0   0.0  0.0000  0.000000  0.000000   0.0  0.0000  \n",
       "...    ...   ...   ...   ...   ...     ...       ...       ...   ...     ...  \n",
       "6265   0.0   0.0   0.0   0.0   0.0  0.0000  0.000000  0.000000   0.0  0.0000  \n",
       "6266   0.0   0.0   0.0   0.0   0.0  0.0256  0.000000  0.000000   0.0  0.0000  \n",
       "6267   0.0   0.0   0.0   0.0   0.0  0.0256  0.000000  0.000000   0.0  0.0000  \n",
       "6268   0.0   0.0   0.0   0.0   0.0  0.0256  0.000000  0.000000   0.0  0.0000  \n",
       "6269   0.0   0.0   0.0   0.0   0.0  0.0000  0.000000  0.000000   0.0  0.0435  \n",
       "\n",
       "[6270 rows x 3653 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_TF_matrix(doc_clean, useTransfertDict=True):\n",
    "    \"\"\"\n",
    "    Input  : clean document\n",
    "    Purpose: get the term frequency matrix from a corpus\n",
    "    Output : Document Term Frequency Matrix\n",
    "    \"\"\"\n",
    "    # Creating the term dictionary of our courpus, where every unique term is assigned an index \n",
    "    dictionary = corpora.Dictionary(doc_clean)\n",
    "\n",
    "    # Converting list of documents (corpus) into Document Term Matrix using dictionary prepared above\n",
    "    return [dictionary.doc2bow(doc) for doc in doc_clean]\n",
    "    \n",
    "def get_normTF_matrix(doc_clean):\n",
    "    \"\"\"\n",
    "    Input  : clean document\n",
    "    Purpose: get the term frequency matrix from a corpus\n",
    "    Output : Document Term Frequency Matrix\n",
    "    \"\"\"\n",
    "    # Creating the term dictionary of our courpus, where every unique term is assigned an index\n",
    "    dictionary = corpora.Dictionary(doc_clean)\n",
    "\n",
    "    # Converting list of documents (corpus) into Document Term Matrix using dictionary prepared above\n",
    "    TFmat = pd.DataFrame(0,\n",
    "                         columns=[i for i in range(len(doc_clean))],\n",
    "                         index=[i for i in range(len(dictionary))]\n",
    "                        )\n",
    "    i=0\n",
    "    for doc in doc_clean:\n",
    "        vec = [0 for i in range(len(dictionary))]\n",
    "        for wrd, freq in dictionary.doc2bow(doc):\n",
    "            vec[wrd] = round(freq / len(doc), 4) # normalization\n",
    "        TFmat[i] = vec\n",
    "        i+=1\n",
    "    return TFmat\n",
    "\n",
    "def get_IDF_matrix(doc_clean, useTransfertDict=True):\n",
    "    \"\"\"\n",
    "    Input  : clean document\n",
    "    Purpose: create IDF matrix\n",
    "    Output : IDF Matrix\n",
    "    \"\"\"\n",
    "    # Create vocabulary\n",
    "    if useTransfertDict:\n",
    "        vocabulary = common_dictionary\n",
    "    else:\n",
    "        vocabulary = get_dictionary(doc_clean)\n",
    "    \n",
    "    # Creating tf matrix\n",
    "    TF_matrix = get_TF_matrix(doc_clean)\n",
    "    \n",
    "    # Converting list of documents (corpus) into IDF Matrix using dictionary prepared above\n",
    "    word_idf_values = {i:0 for i in list(vocabulary.keys())}\n",
    "    \n",
    "    for w in range(len(vocabulary)): \n",
    "        for doc in TF_matrix:\n",
    "            for wrd in doc:\n",
    "                if w == wrd[0]:\n",
    "                    word_idf_values[w]+=1\n",
    "        word_idf_values[w] = np.log(len(doc_clean) / (1 + word_idf_values[w]))\n",
    "    return word_idf_values\n",
    "\n",
    "def get_TFIDF_matrix(doc_clean):\n",
    "    \"\"\"\n",
    "    Input  : clean document\n",
    "    Purpose: create TFIDF matrix\n",
    "    Output : TFIDF Matrix\n",
    "    \"\"\"\n",
    "    # Creating tf matrix\n",
    "    TF_matrix = get_normTF_matrix(doc_clean)\n",
    "\n",
    "    # Create IDF\n",
    "    IDF_vector = get_IDF_matrix(doc_clean)\n",
    "\n",
    "    # initialize TFIDF as TF\n",
    "    TFIDF_matrix = TF_matrix.copy()\n",
    "    \n",
    "    # update TF Matrix using IDF term.\n",
    "    for doc in range(len(doc_clean)):\n",
    "        for word, score in IDF_vector.items():\n",
    "            TFIDF_matrix[doc][word] = TFIDF_matrix[doc][word] * score\n",
    "    return TFIDF_matrix\n",
    "\n",
    "# Compute TFIDF matrix\n",
    "corpus_TFIDFmatrix = get_TFIDF_matrix(df.clean_description)\n",
    "corpus_TFIDFmatrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSI Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MFYnt8QmYBNa"
   },
   "outputs": [],
   "source": [
    "def create_gensim_lsi_model(clean_documents_list, k=None):\n",
    "    \"\"\"\n",
    "    Input  : clean document, dictionary\n",
    "    Purpose: create LSI model (Latent Semantic Indexing) from corpus and dictionary\n",
    "    Output : return LSI model\n",
    "    \"\"\"\n",
    "    \n",
    "    # LSI model consists of Singular Value Decomposition (SVD) of\n",
    "    # Term Document Matrix M: M = T x S x D\n",
    "    # and dimensionality reductions of T, S and D' (\"Derivation\")\n",
    "    \n",
    "    dictionary = get_dictionary(clean_documents_list)\n",
    "    corpus = get_TF_matrix(clean_documents_list)\n",
    "    \n",
    "    if k is not None:\n",
    "        lsi_model = LsiModel(\n",
    "                corpus = corpus,\n",
    "                id2word = dictionary,\n",
    "                num_topics = int(k)\n",
    "                )\n",
    "    else:\n",
    "            lsi_model = LsiModel(\n",
    "            corpus = corpus,\n",
    "            id2word = dictionary \n",
    "            )\n",
    "    \n",
    "    print(); print(); print(\"=\"*20, \"Training LSI model report\", \"=\"*20); print() # for output design\n",
    "    print(\"Initial TF matrix (NwordsXNdocuments): \")\n",
    "    \n",
    "    TF = []\n",
    "    for x in corpus:\n",
    "        wrds = [0 for i in range(len(dictionary))]\n",
    "        for i, j in x:\n",
    "            wrds[i] = j \n",
    "        TF.append(wrds)\n",
    "\n",
    "    print(pd.np.transpose(TF))\n",
    "    print()\n",
    "    print(\"Derivation of Term Matrix T of Training Document Word Stems: \")\n",
    "    print(lsi_model.get_topics())\n",
    "    print()\n",
    "    \n",
    "    #Derivation of Term Document Matrix of Training Document Word Stems = M' * [Derivation of T]\n",
    "    print(\"LSI Vectors of Training Document Word Stems: \")\n",
    "    print([lsi_model[document_word_stems] for document_word_stems in corpus])\n",
    "    print(\"=\"*70); print(); print()\n",
    "    return lsi_model\n",
    "\n",
    "def get_lsi_vector(lsi_model, clean_text):\n",
    "    return lsi_model[dictionary.doc2bow(clean_text)]\n",
    "\n",
    "def select_optimal_k_value(singular_values, significativity=75):\n",
    "    singular_values.sort()\n",
    "    lsi_model.projection.s = lsi_model.projection.s[::-1]\n",
    "    sum_of_singular_values = sum(singular_values)\n",
    "    s = 0; k = 0 \n",
    "    while((s < significativity * sum_of_singular_values / 100) and \n",
    "          k < max(2, len(singular_values) - 2)):\n",
    "        s+=singular_values[k]\n",
    "        k+=1\n",
    "    return(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 680
    },
    "colab_type": "code",
    "id": "VknlN6Rza1IM",
    "outputId": "03841a72-2451-4c4a-9dbe-6349a4a33114"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "==================== Training LSI model report ====================\n",
      "\n",
      "Initial TF matrix (NwordsXNdocuments): \n",
      "[[4 0 0 ... 2 0 0]\n",
      " [1 0 0 ... 0 0 0]\n",
      " [1 0 0 ... 0 0 0]\n",
      " ...\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 1]]\n",
      "\n",
      "Derivation of Term Matrix T of Training Document Word Stems: \n",
      "[[ 7.23705607e-02  6.97908324e-02  4.70463610e-03 ...  1.39348665e-04\n",
      "   1.39348665e-04  1.34751860e-04]\n",
      " [ 3.75794766e-02  5.24634529e-02 -1.46296916e-02 ... -1.61322341e-05\n",
      "  -1.61322341e-05  1.72675783e-05]\n",
      " [ 1.22073780e-02 -8.25197875e-02  1.07036888e-03 ... -4.52808118e-04\n",
      "  -4.52808118e-04 -2.28038634e-04]\n",
      " ...\n",
      " [ 5.94529913e-03  1.90138412e-02  2.10619885e-02 ...  3.13933687e-03\n",
      "   3.13933687e-03 -1.75720093e-03]\n",
      " [-9.51369553e-03 -1.21448589e-02 -2.14777424e-02 ...  3.23036416e-04\n",
      "   3.23036416e-04  5.08535382e-04]\n",
      " [-5.69904118e-03 -2.18282804e-03  3.90754960e-03 ... -3.77929032e-04\n",
      "  -3.77929032e-04  4.19763646e-04]]\n",
      "\n",
      "LSI Vectors of Training Document Word Stems: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub data rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# train lsi model (init)\n",
    "lsi_model = create_gensim_lsi_model(df.clean_description, k=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class color:\n",
    "    PURPLE = '\\033[95m'\n",
    "    CYAN = '\\033[96m'\n",
    "    DARKCYAN = '\\033[36m'\n",
    "    BLUE = '\\033[94m'\n",
    "    GREEN = '\\033[92m'\n",
    "    YELLOW = '\\033[93m'\n",
    "    RED = '\\033[91m'\n",
    "    BOLD = '\\033[1m'\n",
    "    UNDERLINE = '\\033[4m'\n",
    "    END = '\\033[0m'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "oppQlBu_ahEJ",
    "outputId": "a2d97799-92df-4b8b-fef5-d39d83960e82",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m Dimension reduction - optimal k is:  \u001b[0m 175\n"
     ]
    }
   ],
   "source": [
    "optimal_k = select_optimal_k_value(lsi_model.projection.s, significativity=75)\n",
    "print(color.BOLD, \"Dimension reduction - optimal k is: \", color.END, optimal_k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "==================== Training LSI model report ====================\n",
      "\n",
      "Initial TF matrix (NwordsXNdocuments): \n",
      "[[4 0 0 ... 2 0 0]\n",
      " [1 0 0 ... 0 0 0]\n",
      " [1 0 0 ... 0 0 0]\n",
      " ...\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 1]]\n",
      "\n",
      "Derivation of Term Matrix T of Training Document Word Stems: \n",
      "[[ 7.23705879e-02  6.97906795e-02  4.70473752e-03 ...  1.39348779e-04\n",
      "   1.39348779e-04  1.34747845e-04]\n",
      " [-3.75782983e-02 -5.24650501e-02  1.46274887e-02 ...  1.59579103e-05\n",
      "   1.59579103e-05 -1.68235266e-05]\n",
      " [ 1.22030008e-02 -8.25370239e-02  1.09320727e-03 ... -4.50397972e-04\n",
      "  -4.50397972e-04 -2.25314951e-04]\n",
      " ...\n",
      " [ 7.13572006e-04  4.45298318e-02  1.15942290e-02 ...  3.26972889e-04\n",
      "   3.26972889e-04  1.76289808e-04]\n",
      " [-4.37734403e-03 -4.61756133e-03  4.84904691e-03 ...  2.58569631e-03\n",
      "   2.58569631e-03 -1.71858652e-05]\n",
      " [ 5.30136764e-04  7.24309674e-03  7.62407484e-03 ... -1.38575750e-04\n",
      "  -1.38575750e-04 -4.00436337e-04]]\n",
      "\n",
      "LSI Vectors of Training Document Word Stems: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub data rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# train lsi model (optimal_k)\n",
    "lsi_model = create_gensim_lsi_model(df.clean_description, k=optimal_k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display singular values\n",
    "# lsi_model.projection.s\n",
    "\n",
    "# display left singular vectors\n",
    "# lsi_model.projection.u\n",
    "\n",
    "# display right singular vectors (can be reconstructed if needed).\n",
    "#print(color.BOLD, \"LSI Vectors of Training Document Word Stems: \", color.END)\n",
    "#[lsi_model[d] for d in get_TF_matrix(df.clean_description)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word2Vec Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Thomas/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:8: DeprecationWarning: Call to deprecated `iter` (Attribute will be removed in 4.0.0, use self.epochs instead).\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "corpus = df.clean_description.tolist()\n",
    "max_embedding = 100\n",
    "\n",
    "path = get_tmpfile(\"word2vec.model\")\n",
    "\n",
    "model = gensim.models.Word2Vec(size=max_embedding, window=3, min_count=5, workers=4, seed=1, iter=50)\n",
    "model.build_vocab(corpus[:3000])\n",
    "model.train(corpus[:3000], total_examples=model.corpus_count, epochs=model.iter)\n",
    "model.save(\"word2vec.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = gensim.models.Word2Vec.load(\"word2vec.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('browni', 0.5312399864196777),\n",
       " ('chocol', 0.5199911594390869),\n",
       " ('bread', 0.510167121887207),\n",
       " ('pistachio', 0.4986397624015808),\n",
       " ('tiramisu', 0.4875122904777527),\n",
       " ('mango', 0.4767257571220398),\n",
       " ('bitter', 0.47206079959869385),\n",
       " ('chees', 0.468998521566391),\n",
       " ('tomato', 0.4586534798145294),\n",
       " ('cream', 0.4545954167842865)]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.most_similar(\"dessert\", topn=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100,)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv['use'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_matrix = dict()\n",
    "\n",
    "for word in model.wv.vocab.keys():\n",
    "    embedding_matrix[word] = list(model.wv[word])\n",
    "    \n",
    "embedding_matrix = pd.DataFrame(embedding_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>use</th>\n",
       "      <th>restaur</th>\n",
       "      <th>year</th>\n",
       "      <th>chef</th>\n",
       "      <th>excel</th>\n",
       "      <th>alway</th>\n",
       "      <th>good</th>\n",
       "      <th>portion</th>\n",
       "      <th>food</th>\n",
       "      <th>fresh</th>\n",
       "      <th>...</th>\n",
       "      <th>martin</th>\n",
       "      <th>tunisian</th>\n",
       "      <th>cultur</th>\n",
       "      <th>moniz</th>\n",
       "      <th>portugues</th>\n",
       "      <th>masti</th>\n",
       "      <th>skewd</th>\n",
       "      <th>beehiv</th>\n",
       "      <th>gezi</th>\n",
       "      <th>kulcha</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>-0.788264</td>\n",
       "      <td>0.038806</td>\n",
       "      <td>0.183511</td>\n",
       "      <td>-1.572962</td>\n",
       "      <td>-0.188596</td>\n",
       "      <td>-0.130794</td>\n",
       "      <td>-0.005102</td>\n",
       "      <td>0.107818</td>\n",
       "      <td>0.898989</td>\n",
       "      <td>-0.901034</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.088729</td>\n",
       "      <td>-0.017593</td>\n",
       "      <td>0.081196</td>\n",
       "      <td>0.094013</td>\n",
       "      <td>0.096104</td>\n",
       "      <td>0.152688</td>\n",
       "      <td>-0.380789</td>\n",
       "      <td>0.210192</td>\n",
       "      <td>-0.117906</td>\n",
       "      <td>-0.288409</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>-0.194942</td>\n",
       "      <td>0.829009</td>\n",
       "      <td>1.469187</td>\n",
       "      <td>1.423897</td>\n",
       "      <td>0.366503</td>\n",
       "      <td>1.367753</td>\n",
       "      <td>0.533291</td>\n",
       "      <td>-0.994256</td>\n",
       "      <td>0.186483</td>\n",
       "      <td>-0.478119</td>\n",
       "      <td>...</td>\n",
       "      <td>0.029562</td>\n",
       "      <td>0.070191</td>\n",
       "      <td>0.001750</td>\n",
       "      <td>-0.290413</td>\n",
       "      <td>0.012101</td>\n",
       "      <td>0.074410</td>\n",
       "      <td>0.044519</td>\n",
       "      <td>0.224215</td>\n",
       "      <td>-0.253100</td>\n",
       "      <td>0.150007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>-0.144564</td>\n",
       "      <td>0.507244</td>\n",
       "      <td>0.066529</td>\n",
       "      <td>-0.222820</td>\n",
       "      <td>0.460443</td>\n",
       "      <td>-0.192375</td>\n",
       "      <td>-0.334750</td>\n",
       "      <td>1.666057</td>\n",
       "      <td>-0.118697</td>\n",
       "      <td>0.775607</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.136900</td>\n",
       "      <td>-0.052802</td>\n",
       "      <td>0.022676</td>\n",
       "      <td>-0.095946</td>\n",
       "      <td>-0.214366</td>\n",
       "      <td>-0.127279</td>\n",
       "      <td>-0.061020</td>\n",
       "      <td>-0.434667</td>\n",
       "      <td>-0.279612</td>\n",
       "      <td>0.041811</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.018410</td>\n",
       "      <td>0.360076</td>\n",
       "      <td>0.611324</td>\n",
       "      <td>-0.099415</td>\n",
       "      <td>-0.923164</td>\n",
       "      <td>-0.225428</td>\n",
       "      <td>-1.382897</td>\n",
       "      <td>0.030158</td>\n",
       "      <td>-0.638276</td>\n",
       "      <td>-0.992657</td>\n",
       "      <td>...</td>\n",
       "      <td>0.117096</td>\n",
       "      <td>-0.116768</td>\n",
       "      <td>0.045856</td>\n",
       "      <td>0.018902</td>\n",
       "      <td>-0.211426</td>\n",
       "      <td>0.006323</td>\n",
       "      <td>-0.180946</td>\n",
       "      <td>0.245709</td>\n",
       "      <td>-0.118267</td>\n",
       "      <td>-0.403101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.030509</td>\n",
       "      <td>-0.546878</td>\n",
       "      <td>-0.531355</td>\n",
       "      <td>-0.658993</td>\n",
       "      <td>0.480832</td>\n",
       "      <td>-0.339031</td>\n",
       "      <td>0.406634</td>\n",
       "      <td>-0.670564</td>\n",
       "      <td>0.221760</td>\n",
       "      <td>0.171875</td>\n",
       "      <td>...</td>\n",
       "      <td>0.306006</td>\n",
       "      <td>-0.232920</td>\n",
       "      <td>-0.158120</td>\n",
       "      <td>-0.134483</td>\n",
       "      <td>-0.123283</td>\n",
       "      <td>0.014353</td>\n",
       "      <td>-0.250805</td>\n",
       "      <td>0.054812</td>\n",
       "      <td>-0.249806</td>\n",
       "      <td>0.256638</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 1615 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        use   restaur      year      chef     excel     alway      good  \\\n",
       "0 -0.788264  0.038806  0.183511 -1.572962 -0.188596 -0.130794 -0.005102   \n",
       "1 -0.194942  0.829009  1.469187  1.423897  0.366503  1.367753  0.533291   \n",
       "2 -0.144564  0.507244  0.066529 -0.222820  0.460443 -0.192375 -0.334750   \n",
       "3  0.018410  0.360076  0.611324 -0.099415 -0.923164 -0.225428 -1.382897   \n",
       "4  0.030509 -0.546878 -0.531355 -0.658993  0.480832 -0.339031  0.406634   \n",
       "\n",
       "    portion      food     fresh  ...    martin  tunisian    cultur     moniz  \\\n",
       "0  0.107818  0.898989 -0.901034  ... -0.088729 -0.017593  0.081196  0.094013   \n",
       "1 -0.994256  0.186483 -0.478119  ...  0.029562  0.070191  0.001750 -0.290413   \n",
       "2  1.666057 -0.118697  0.775607  ... -0.136900 -0.052802  0.022676 -0.095946   \n",
       "3  0.030158 -0.638276 -0.992657  ...  0.117096 -0.116768  0.045856  0.018902   \n",
       "4 -0.670564  0.221760  0.171875  ...  0.306006 -0.232920 -0.158120 -0.134483   \n",
       "\n",
       "   portugues     masti     skewd    beehiv      gezi    kulcha  \n",
       "0   0.096104  0.152688 -0.380789  0.210192 -0.117906 -0.288409  \n",
       "1   0.012101  0.074410  0.044519  0.224215 -0.253100  0.150007  \n",
       "2  -0.214366 -0.127279 -0.061020 -0.434667 -0.279612  0.041811  \n",
       "3  -0.211426  0.006323 -0.180946  0.245709 -0.118267 -0.403101  \n",
       "4  -0.123283  0.014353 -0.250805  0.054812 -0.249806  0.256638  \n",
       "\n",
       "[5 rows x 1615 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_matrix.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fast Text Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_FT = gensim.models.FastText(size=max_embedding, window=3, \n",
    "                                  min_count=5, workers=4, sg=1, seed=1, iter=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Thomas/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:2: DeprecationWarning: Call to deprecated `iter` (Attribute will be removed in 4.0.0, use self.epochs instead).\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "model_FT.build_vocab(corpus[:3000])\n",
    "model_FT.train(corpus[:3000], total_examples=model.corpus_count, epochs=model.iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('desert', 0.6242941617965698),\n",
       " ('chocol', 0.5313356518745422),\n",
       " ('mango', 0.5165298581123352),\n",
       " ('appet', 0.49837106466293335),\n",
       " ('sorbet', 0.48578161001205444),\n",
       " ('appetis', 0.46856147050857544),\n",
       " ('appetit', 0.4657207429409027),\n",
       " ('appl', 0.458345890045166),\n",
       " ('fondant', 0.44326961040496826),\n",
       " ('pud', 0.436087965965271)]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_FT.wv.most_similar(\"dessert\", topn=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100,)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_FT.wv['use'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>use</th>\n",
       "      <th>restaur</th>\n",
       "      <th>year</th>\n",
       "      <th>chef</th>\n",
       "      <th>excel</th>\n",
       "      <th>alway</th>\n",
       "      <th>good</th>\n",
       "      <th>portion</th>\n",
       "      <th>food</th>\n",
       "      <th>fresh</th>\n",
       "      <th>...</th>\n",
       "      <th>martin</th>\n",
       "      <th>tunisian</th>\n",
       "      <th>cultur</th>\n",
       "      <th>moniz</th>\n",
       "      <th>portugues</th>\n",
       "      <th>masti</th>\n",
       "      <th>skewd</th>\n",
       "      <th>beehiv</th>\n",
       "      <th>gezi</th>\n",
       "      <th>kulcha</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>-0.441920</td>\n",
       "      <td>0.198596</td>\n",
       "      <td>0.544554</td>\n",
       "      <td>0.627261</td>\n",
       "      <td>0.194475</td>\n",
       "      <td>0.008950</td>\n",
       "      <td>0.201402</td>\n",
       "      <td>-0.031286</td>\n",
       "      <td>0.041259</td>\n",
       "      <td>0.038559</td>\n",
       "      <td>...</td>\n",
       "      <td>0.349276</td>\n",
       "      <td>0.288220</td>\n",
       "      <td>0.265725</td>\n",
       "      <td>0.157498</td>\n",
       "      <td>-0.132034</td>\n",
       "      <td>0.122696</td>\n",
       "      <td>0.509829</td>\n",
       "      <td>0.045211</td>\n",
       "      <td>0.366371</td>\n",
       "      <td>0.040678</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>-0.282004</td>\n",
       "      <td>-0.371620</td>\n",
       "      <td>-0.625822</td>\n",
       "      <td>-0.024100</td>\n",
       "      <td>-0.125392</td>\n",
       "      <td>0.169849</td>\n",
       "      <td>-0.199216</td>\n",
       "      <td>-0.472755</td>\n",
       "      <td>-0.359557</td>\n",
       "      <td>-0.245306</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.346349</td>\n",
       "      <td>-0.360504</td>\n",
       "      <td>-0.362741</td>\n",
       "      <td>-0.469855</td>\n",
       "      <td>-0.535890</td>\n",
       "      <td>-0.681524</td>\n",
       "      <td>-0.501094</td>\n",
       "      <td>-0.260336</td>\n",
       "      <td>-0.556496</td>\n",
       "      <td>-0.256627</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.121368</td>\n",
       "      <td>0.004188</td>\n",
       "      <td>0.171993</td>\n",
       "      <td>-0.036525</td>\n",
       "      <td>0.141909</td>\n",
       "      <td>0.207596</td>\n",
       "      <td>0.166569</td>\n",
       "      <td>0.051700</td>\n",
       "      <td>0.076973</td>\n",
       "      <td>-0.194531</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.114003</td>\n",
       "      <td>0.022822</td>\n",
       "      <td>0.094328</td>\n",
       "      <td>0.416586</td>\n",
       "      <td>0.021230</td>\n",
       "      <td>0.659880</td>\n",
       "      <td>0.165043</td>\n",
       "      <td>-0.161475</td>\n",
       "      <td>0.105740</td>\n",
       "      <td>-0.062406</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>-0.244560</td>\n",
       "      <td>-0.020557</td>\n",
       "      <td>0.081020</td>\n",
       "      <td>0.188281</td>\n",
       "      <td>-0.037971</td>\n",
       "      <td>-0.012920</td>\n",
       "      <td>-0.002355</td>\n",
       "      <td>-0.260551</td>\n",
       "      <td>-0.122618</td>\n",
       "      <td>-0.501384</td>\n",
       "      <td>...</td>\n",
       "      <td>0.554781</td>\n",
       "      <td>-0.088594</td>\n",
       "      <td>-0.025938</td>\n",
       "      <td>0.091139</td>\n",
       "      <td>0.157923</td>\n",
       "      <td>-0.067108</td>\n",
       "      <td>0.552165</td>\n",
       "      <td>-0.095319</td>\n",
       "      <td>-0.197058</td>\n",
       "      <td>0.005904</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.333622</td>\n",
       "      <td>-0.238360</td>\n",
       "      <td>0.500754</td>\n",
       "      <td>0.225712</td>\n",
       "      <td>-0.251495</td>\n",
       "      <td>-0.132040</td>\n",
       "      <td>-0.311705</td>\n",
       "      <td>0.114618</td>\n",
       "      <td>-0.034449</td>\n",
       "      <td>-0.158335</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.061770</td>\n",
       "      <td>-0.427533</td>\n",
       "      <td>-0.001008</td>\n",
       "      <td>0.655511</td>\n",
       "      <td>-0.262465</td>\n",
       "      <td>0.058301</td>\n",
       "      <td>-0.436308</td>\n",
       "      <td>-0.025215</td>\n",
       "      <td>-0.217383</td>\n",
       "      <td>-0.002148</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 1615 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        use   restaur      year      chef     excel     alway      good  \\\n",
       "0 -0.441920  0.198596  0.544554  0.627261  0.194475  0.008950  0.201402   \n",
       "1 -0.282004 -0.371620 -0.625822 -0.024100 -0.125392  0.169849 -0.199216   \n",
       "2  0.121368  0.004188  0.171993 -0.036525  0.141909  0.207596  0.166569   \n",
       "3 -0.244560 -0.020557  0.081020  0.188281 -0.037971 -0.012920 -0.002355   \n",
       "4  0.333622 -0.238360  0.500754  0.225712 -0.251495 -0.132040 -0.311705   \n",
       "\n",
       "    portion      food     fresh  ...    martin  tunisian    cultur     moniz  \\\n",
       "0 -0.031286  0.041259  0.038559  ...  0.349276  0.288220  0.265725  0.157498   \n",
       "1 -0.472755 -0.359557 -0.245306  ... -0.346349 -0.360504 -0.362741 -0.469855   \n",
       "2  0.051700  0.076973 -0.194531  ... -0.114003  0.022822  0.094328  0.416586   \n",
       "3 -0.260551 -0.122618 -0.501384  ...  0.554781 -0.088594 -0.025938  0.091139   \n",
       "4  0.114618 -0.034449 -0.158335  ... -0.061770 -0.427533 -0.001008  0.655511   \n",
       "\n",
       "   portugues     masti     skewd    beehiv      gezi    kulcha  \n",
       "0  -0.132034  0.122696  0.509829  0.045211  0.366371  0.040678  \n",
       "1  -0.535890 -0.681524 -0.501094 -0.260336 -0.556496 -0.256627  \n",
       "2   0.021230  0.659880  0.165043 -0.161475  0.105740 -0.062406  \n",
       "3   0.157923 -0.067108  0.552165 -0.095319 -0.197058  0.005904  \n",
       "4  -0.262465  0.058301 -0.436308 -0.025215 -0.217383 -0.002148  \n",
       "\n",
       "[5 rows x 1615 columns]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_matrix_FT = dict()\n",
    "\n",
    "for word in model_FT.wv.vocab.keys():\n",
    "    embedding_matrix_FT[word] = list(model_FT.wv[word])\n",
    "    \n",
    "embedding_matrix_FT = pd.DataFrame(embedding_matrix_FT)\n",
    "\n",
    "embedding_matrix_FT.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "09GdxVw_bPse"
   },
   "source": [
    "# Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GQlc4k0FbRBy"
   },
   "source": [
    "## Split train/test data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start by splitting the data into a train set and a validation set, in order to train and validate our models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "smT7mu7La1Y9",
    "outputId": "ff6e0dc7-df7a-4034-c5d0-3e61dd7b185b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split train/test:  (2435, 7) VS (1218, 7)\n"
     ]
    }
   ],
   "source": [
    "n = len(df)\n",
    "df.sample(n=n, random_state=16) #shuffle\n",
    "n = int(2 * n / 3)\n",
    "df_dataset_train = df[:n]\n",
    "df_dataset_test = df[n:]\n",
    "print(\"Split train/test: \", df_dataset_train.shape, \"VS\", df_dataset_test.shape)\n",
    "\n",
    "corpus_TFmatrix_train = get_TF_matrix(df_dataset_train.clean_description)\n",
    "corpus_TFmatrix_test = get_TF_matrix(df_dataset_test.clean_description)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cosine Distance Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cosine Distance on LSI embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 88
    },
    "colab_type": "code",
    "id": "DiYKMUYCbS89",
    "outputId": "760e9641-65e5-4fcc-bca2-b56a6933c7d7"
   },
   "outputs": [],
   "source": [
    "def distance_classifier_cosine_traning(lsi_vector_trainDB):\n",
    "    \"\"\"\n",
    "    Input  : LSI vectors\n",
    "    Purpose: calculate cosine similarity matrix for all training document LSI vectors\n",
    "    Output : return similarity matrix\n",
    "    \"\"\"\n",
    "    return similarities.MatrixSimilarity(lsi_vector_trainDB)\n",
    "\n",
    "def distance_classifier_cosine_test(classification_model, training_data, test_doc_lsi_vector, N=1):\n",
    "    \"\"\"\n",
    "    Input  : trained classifier model, the training data (list of descriptions),\n",
    "             lsi vectors of a document and N nearest document in the training data base\n",
    "    Purpose: calculate cosine similarity matrix against all training document LSI vectors\n",
    "    Output : return nearest N document and classes\n",
    "    \"\"\"\n",
    "    cosine_similarities = classification_model[test_doc_lsi_vector]\n",
    "    most_similar_document_test = training_data[np.argmax(cosine_similarities)]\n",
    "    return most_similar_document_test\n",
    "\n",
    "def reco_rate(ref_labels, predicted_labels):\n",
    "    commun_labels = (pd.np.array(ref_labels)==pd.np.array(predicted_labels)).sum()\n",
    "    return 100 * commun_labels / len(ref_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<gensim.similarities.docsim.MatrixSimilarity at 0x1a2f8d1e80>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train classification model\n",
    "\n",
    "classification_model = distance_classifier_cosine_traning(lsi_model[corpus_TFmatrix_train])\n",
    "classification_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "bXsIipQFbWGl",
    "outputId": "f3b9f5a7-06c4-4c0b-9818-7f9ae375f5c2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mClassifier performances on train DB: 100.00 %\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# test on train DB\n",
    "\n",
    "predicted_class = [distance_classifier_cosine_test(classification_model, \n",
    "                                df_dataset_train.Rating, \n",
    "                                get_lsi_vector(lsi_model, df_dataset_train.clean_description.iloc[i]))\n",
    "                                for i in range(df_dataset_train.shape[0])]\n",
    "\n",
    "print(color.BOLD + \"Classifier performances on train DB: %.2f\" \n",
    "      % reco_rate(df_dataset_train.Rating, predicted_class), \"%\" + color.END)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "J30CDQm-cL-s",
    "outputId": "422fb803-b808-4b28-f844-6fc3eea1c702"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mClassifier performances on test DB: 68.56 %\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# test on test DB\n",
    "\n",
    "predicted_class_test = [distance_classifier_cosine_test(classification_model, \n",
    "                                 df_dataset_train.Rating, \n",
    "                                 get_lsi_vector(lsi_model, \n",
    "                                               df_dataset_test.clean_description.iloc[i]\n",
    "                                              ))\n",
    "                   for i in range(df_dataset_test.shape[0])]\n",
    "\n",
    "print(color.BOLD + \"Classifier performances on test DB: %.2f\" \n",
    "      % (reco_rate(df_dataset_test.Rating, predicted_class_test)), \"%\" + color.END)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNN on LSI embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mmbdTklSuIMc"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense,GRU,LSTM\n",
    "from keras.optimizers import Adam\n",
    "from keras.layers import Dropout,Bidirectional,Conv1D,Conv2D,GlobalMaxPooling1D,GlobalMaxPooling2D,MaxPooling1D,Flatten,BatchNormalization,Embedding\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2435, 1, 175)\n",
      "(1218, 1, 175)\n",
      "(2435, 6)\n",
      "(1218, 6)\n"
     ]
    }
   ],
   "source": [
    "# For the CNN:\n",
    "# `x_train` and `x_val` = embedded reviews (reformated for CNN in the following lines of code)\n",
    "# `y_train` and `y_val` = ratings (one-hot encoded)\n",
    "\n",
    "x_train = [get_lsi_vector(lsi_model, df_dataset_train.clean_description.iloc[i]) for i in range(df_dataset_train.shape[0])]\n",
    "# previous line is equiv to: x_train = np.asarray(lsi_model[corpus_TFmatrix_train])\n",
    "x_train = np.asarray([[x_train[j][i][1] for i in range(len(x_train[j]))] for j in range(len(x_train))])\n",
    "x_train = x_train.reshape((x_train.shape[0], 1, x_train.shape[1])) # because CNN has to be fed a 3D object\n",
    "print(x_train.shape)\n",
    "\n",
    "x_val = [get_lsi_vector(lsi_model, df_dataset_test.clean_description.iloc[i]) for i in range(df_dataset_test.shape[0])]\n",
    "x_val = np.asarray([[x_val[j][i][1] for i in range(len(x_val[j]))] for j in range(len(x_val))])\n",
    "x_val = x_val.reshape((x_val.shape[0], 1, x_val.shape[1]))\n",
    "print(x_val.shape)\n",
    "\n",
    "y_train = np.asarray(df_dataset_train.Rating)\n",
    "y_train = to_categorical(y_train)\n",
    "print(y_train.shape)\n",
    "\n",
    "y_val = np.asarray(df_dataset_test.Rating)\n",
    "y_val = to_categorical(y_val)\n",
    "print(y_val.shape)\n",
    "\n",
    "num_classes = y_train.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_9 (Conv1D)            (None, 1, 64)             33664     \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_9 (Glob (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_15 (Dense)             (None, 128)               8320      \n",
      "_________________________________________________________________\n",
      "dropout_9 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_8 (Batch (None, 128)               512       \n",
      "_________________________________________________________________\n",
      "dense_16 (Dense)             (None, 6)                 774       \n",
      "=================================================================\n",
      "Total params: 43,270\n",
      "Trainable params: 43,014\n",
      "Non-trainable params: 256\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_CNN = Sequential()\n",
    "\n",
    "model_CNN.add(Conv1D(64,input_shape=(1, 175),kernel_size=3,padding='same',activation='relu',strides=1))\n",
    "model_CNN.add(GlobalMaxPooling1D())\n",
    "model_CNN.add(Dense(128,activation='relu'))\n",
    "model_CNN.add(Dropout(0.2))\n",
    "model_CNN.add(BatchNormalization())\n",
    "model_CNN.add(Dense(num_classes,activation='sigmoid'))\n",
    "\n",
    "model_CNN.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
    "model_CNN.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 2435 samples, validate on 1218 samples\n",
      "Epoch 1/4\n",
      "2435/2435 [==============================] - 5s 2ms/step - loss: 0.6728 - acc: 0.6291 - val_loss: 0.5064 - val_acc: 0.8243\n",
      "Epoch 2/4\n",
      "2435/2435 [==============================] - 1s 446us/step - loss: 0.3664 - acc: 0.8861 - val_loss: 0.2492 - val_acc: 0.9302\n",
      "Epoch 3/4\n",
      "2435/2435 [==============================] - 1s 447us/step - loss: 0.2251 - acc: 0.9242 - val_loss: 0.2000 - val_acc: 0.9324\n",
      "Epoch 4/4\n",
      "2435/2435 [==============================] - 1s 451us/step - loss: 0.1813 - acc: 0.9320 - val_loss: 0.1906 - val_acc: 0.9328\n"
     ]
    }
   ],
   "source": [
    "early_stop = EarlyStopping(monitor = \"val_loss\", mode = \"min\", patience = 3)\n",
    "\n",
    "history_CNN = model_CNN.fit(x_train, y_train, \n",
    "                            validation_data=(x_val, y_val),\n",
    "                            epochs=4,\n",
    "                            batch_size=32, \n",
    "                            verbose=1,\n",
    "                            callbacks = [early_stop])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accuracy is much higher than with a simpler model (cosine distance)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNN on Word2Vec embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>use</th>\n",
       "      <th>restaur</th>\n",
       "      <th>year</th>\n",
       "      <th>chef</th>\n",
       "      <th>excel</th>\n",
       "      <th>alway</th>\n",
       "      <th>good</th>\n",
       "      <th>portion</th>\n",
       "      <th>food</th>\n",
       "      <th>fresh</th>\n",
       "      <th>...</th>\n",
       "      <th>martin</th>\n",
       "      <th>tunisian</th>\n",
       "      <th>cultur</th>\n",
       "      <th>moniz</th>\n",
       "      <th>portugues</th>\n",
       "      <th>masti</th>\n",
       "      <th>skewd</th>\n",
       "      <th>beehiv</th>\n",
       "      <th>gezi</th>\n",
       "      <th>kulcha</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>-0.429325</td>\n",
       "      <td>-0.240608</td>\n",
       "      <td>0.424857</td>\n",
       "      <td>-0.125756</td>\n",
       "      <td>0.038207</td>\n",
       "      <td>-0.202852</td>\n",
       "      <td>-0.076771</td>\n",
       "      <td>0.409456</td>\n",
       "      <td>0.211813</td>\n",
       "      <td>-0.145445</td>\n",
       "      <td>...</td>\n",
       "      <td>0.076039</td>\n",
       "      <td>-0.075377</td>\n",
       "      <td>-0.119985</td>\n",
       "      <td>0.016004</td>\n",
       "      <td>0.012585</td>\n",
       "      <td>0.014679</td>\n",
       "      <td>-0.009203</td>\n",
       "      <td>0.191222</td>\n",
       "      <td>-0.227424</td>\n",
       "      <td>-0.038551</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.001536</td>\n",
       "      <td>-0.160717</td>\n",
       "      <td>-0.022637</td>\n",
       "      <td>0.471180</td>\n",
       "      <td>0.155363</td>\n",
       "      <td>0.592102</td>\n",
       "      <td>0.332712</td>\n",
       "      <td>-1.040051</td>\n",
       "      <td>0.072936</td>\n",
       "      <td>0.311472</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.115206</td>\n",
       "      <td>-0.301491</td>\n",
       "      <td>0.076295</td>\n",
       "      <td>0.021358</td>\n",
       "      <td>-0.210045</td>\n",
       "      <td>-0.058071</td>\n",
       "      <td>0.069347</td>\n",
       "      <td>-0.024982</td>\n",
       "      <td>0.080424</td>\n",
       "      <td>-0.143771</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.551598</td>\n",
       "      <td>0.228602</td>\n",
       "      <td>0.469899</td>\n",
       "      <td>0.354883</td>\n",
       "      <td>0.200154</td>\n",
       "      <td>0.138538</td>\n",
       "      <td>0.865810</td>\n",
       "      <td>0.127472</td>\n",
       "      <td>0.146347</td>\n",
       "      <td>0.060945</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.251848</td>\n",
       "      <td>0.167571</td>\n",
       "      <td>-0.132739</td>\n",
       "      <td>-0.129528</td>\n",
       "      <td>0.014269</td>\n",
       "      <td>-0.098752</td>\n",
       "      <td>0.160860</td>\n",
       "      <td>-0.161444</td>\n",
       "      <td>0.023455</td>\n",
       "      <td>-0.242818</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>-0.010923</td>\n",
       "      <td>0.254806</td>\n",
       "      <td>0.219923</td>\n",
       "      <td>0.247228</td>\n",
       "      <td>0.077530</td>\n",
       "      <td>0.105497</td>\n",
       "      <td>0.134444</td>\n",
       "      <td>-0.844041</td>\n",
       "      <td>0.066450</td>\n",
       "      <td>0.025498</td>\n",
       "      <td>...</td>\n",
       "      <td>0.143763</td>\n",
       "      <td>-0.034120</td>\n",
       "      <td>-0.125462</td>\n",
       "      <td>0.099024</td>\n",
       "      <td>-0.016786</td>\n",
       "      <td>0.007096</td>\n",
       "      <td>0.120283</td>\n",
       "      <td>-0.044866</td>\n",
       "      <td>0.266713</td>\n",
       "      <td>0.017892</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.247785</td>\n",
       "      <td>-0.121593</td>\n",
       "      <td>0.001933</td>\n",
       "      <td>-0.499855</td>\n",
       "      <td>0.236961</td>\n",
       "      <td>-0.052990</td>\n",
       "      <td>0.256099</td>\n",
       "      <td>0.300218</td>\n",
       "      <td>0.127449</td>\n",
       "      <td>0.614651</td>\n",
       "      <td>...</td>\n",
       "      <td>0.050362</td>\n",
       "      <td>-0.044157</td>\n",
       "      <td>-0.154888</td>\n",
       "      <td>-0.001106</td>\n",
       "      <td>0.038348</td>\n",
       "      <td>0.029304</td>\n",
       "      <td>-0.054640</td>\n",
       "      <td>-0.072185</td>\n",
       "      <td>-0.047177</td>\n",
       "      <td>0.008093</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 1615 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        use   restaur      year      chef     excel     alway      good  \\\n",
       "0 -0.429325 -0.240608  0.424857 -0.125756  0.038207 -0.202852 -0.076771   \n",
       "1  0.001536 -0.160717 -0.022637  0.471180  0.155363  0.592102  0.332712   \n",
       "2  0.551598  0.228602  0.469899  0.354883  0.200154  0.138538  0.865810   \n",
       "3 -0.010923  0.254806  0.219923  0.247228  0.077530  0.105497  0.134444   \n",
       "4  0.247785 -0.121593  0.001933 -0.499855  0.236961 -0.052990  0.256099   \n",
       "\n",
       "    portion      food     fresh  ...    martin  tunisian    cultur     moniz  \\\n",
       "0  0.409456  0.211813 -0.145445  ...  0.076039 -0.075377 -0.119985  0.016004   \n",
       "1 -1.040051  0.072936  0.311472  ... -0.115206 -0.301491  0.076295  0.021358   \n",
       "2  0.127472  0.146347  0.060945  ... -0.251848  0.167571 -0.132739 -0.129528   \n",
       "3 -0.844041  0.066450  0.025498  ...  0.143763 -0.034120 -0.125462  0.099024   \n",
       "4  0.300218  0.127449  0.614651  ...  0.050362 -0.044157 -0.154888 -0.001106   \n",
       "\n",
       "   portugues     masti     skewd    beehiv      gezi    kulcha  \n",
       "0   0.012585  0.014679 -0.009203  0.191222 -0.227424 -0.038551  \n",
       "1  -0.210045 -0.058071  0.069347 -0.024982  0.080424 -0.143771  \n",
       "2   0.014269 -0.098752  0.160860 -0.161444  0.023455 -0.242818  \n",
       "3  -0.016786  0.007096  0.120283 -0.044866  0.266713  0.017892  \n",
       "4   0.038348  0.029304 -0.054640 -0.072185 -0.047177  0.008093  \n",
       "\n",
       "[5 rows x 1615 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_matrix.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first embed the clean review descriptions from the train set to build `x_train`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_words = 20\n",
    "max_embedding = 100 #see 'Word2Vec Embedding' section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2435, 20, 100)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# keeping only the 20 first words of each clean description (i.e. each review)\n",
    "x_train = [df_dataset_train.clean_description.iloc[i][:max_words] for i in range(df_dataset_train.shape[0])]\n",
    "\n",
    "# embedding each word in each review, and padding with 0 vectors if review is shorter than 20 words\n",
    "x_train_w2v = []\n",
    "for i in range(len(x_train)):\n",
    "    x = np.array([np.array(embedding_matrix[word]) for word in x_train[i] if word in embedding_matrix.columns])\n",
    "    while x.shape[0] < max_words:\n",
    "        x = np.append(x, [np.zeros(100)], axis=0)\n",
    "    x_train_w2v.append(x)\n",
    "\n",
    "# converting as array and checking shape\n",
    "x_train = np.array(x_train_w2v)\n",
    "x_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We do the same with the test set to build `x_val`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1218, 20, 100)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# keeping only the 20 first words of each clean description (i.e. each review)\n",
    "x_val = [df_dataset_test.clean_description.iloc[i][:max_words] for i in range(df_dataset_test.shape[0])]\n",
    "\n",
    "# embedding each word in each review, and padding with 0 vectors if review is shorter than 20 words\n",
    "x_val_w2v = []\n",
    "for i in range(len(x_val)):\n",
    "    x = np.array([np.array(embedding_matrix[word]) for word in x_val[i] if word in embedding_matrix.columns])\n",
    "    while x.shape[0] < max_words:\n",
    "        x = np.reshape(x, (x.shape[0],max_embedding))\n",
    "        x = np.concatenate((x, [np.zeros(max_embedding)]), axis=0)\n",
    "    x_val_w2v.append(x)\n",
    "\n",
    "# converting as array and checking shape\n",
    "x_val = np.array(x_val_w2v)\n",
    "x_val.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For `y_train` and `y_val`, we keep the same as in the previous CNN (one-hot encoded ratings):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2435, 6)\n",
      "(1218, 6)\n"
     ]
    }
   ],
   "source": [
    "y_train = np.asarray(df_dataset_train.Rating)\n",
    "y_train = to_categorical(y_train)\n",
    "print(y_train.shape)\n",
    "\n",
    "y_val = np.asarray(df_dataset_test.Rating)\n",
    "y_val = to_categorical(y_val)\n",
    "print(y_val.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now train and test the CNN classifier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_9 (Conv1D)            (None, 20, 128)           38528     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_2 (MaxPooling1 (None, 10, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_10 (Conv1D)           (None, 10, 64)            24640     \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_6 (Glob (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 128)               8320      \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 128)               512       \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 6)                 774       \n",
      "=================================================================\n",
      "Total params: 72,774\n",
      "Trainable params: 72,518\n",
      "Non-trainable params: 256\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_CNN = Sequential()\n",
    "\n",
    "model_CNN.add(Conv1D(128,input_shape=(max_words, max_embedding),kernel_size=3,padding='same',activation='relu',strides=1))\n",
    "model_CNN.add(MaxPooling1D())\n",
    "model_CNN.add(Conv1D(64,kernel_size=3,padding='same',activation='relu',strides=1))\n",
    "model_CNN.add(GlobalMaxPooling1D())\n",
    "model_CNN.add(Dense(128,activation='relu'))\n",
    "model_CNN.add(Dropout(0.2))\n",
    "model_CNN.add(BatchNormalization())\n",
    "model_CNN.add(Dense(num_classes,activation='sigmoid'))\n",
    "\n",
    "model_CNN.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
    "model_CNN.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 2435 samples, validate on 1218 samples\n",
      "Epoch 1/4\n",
      "2435/2435 [==============================] - 7s 3ms/step - loss: 0.6387 - acc: 0.6472 - val_loss: 0.4005 - val_acc: 0.9034\n",
      "Epoch 2/4\n",
      "2435/2435 [==============================] - 3s 1ms/step - loss: 0.3249 - acc: 0.9012 - val_loss: 0.2453 - val_acc: 0.9291\n",
      "Epoch 3/4\n",
      "2435/2435 [==============================] - 3s 1ms/step - loss: 0.2047 - acc: 0.9283 - val_loss: 0.2099 - val_acc: 0.9319\n",
      "Epoch 4/4\n",
      "2435/2435 [==============================] - 3s 1ms/step - loss: 0.1556 - acc: 0.9424 - val_loss: 0.2051 - val_acc: 0.9316\n"
     ]
    }
   ],
   "source": [
    "early_stop = EarlyStopping(monitor = \"val_loss\", mode = \"min\", patience = 3)\n",
    "\n",
    "history_CNN = model_CNN.fit(x_train, y_train, \n",
    "                            validation_data=(x_val, y_val),\n",
    "                            epochs=4,\n",
    "                            batch_size=32, \n",
    "                            verbose=1,\n",
    "                            callbacks = [early_stop])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accuracy is more or less the same as with LSI embeddings. For this last network, adding a 2nd convolutional layer improves performance a little, but it remains just a little lower as the previous CNN based on LSI embeddings. \n",
    "\n",
    "To be more precise about what model works best to predict ratings, we should test our models on a larger set of data (the full scrapped data) and take more time to tune hyperparameters (model architecture, dropout rate, batch normalization, pooling method, kernel size, padding and stride, embedding length, number of words per review, etc. - the list is long...!)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We finally tried LSTM networks, expecting that the LSTM cells would work best at analyzing text thanks to their \"memory\" property."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM with Word2Vec embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_7 (LSTM)                (None, 20, 64)            42240     \n",
      "_________________________________________________________________\n",
      "lstm_8 (LSTM)                (None, 32)                12416     \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 6)                 198       \n",
      "=================================================================\n",
      "Total params: 54,854\n",
      "Trainable params: 54,854\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "max_features = 1615 # nb of words in the vocabulary\n",
    "\n",
    "model_LSTM = Sequential()\n",
    "model_LSTM.add(LSTM(64, input_shape=(max_words, max_embedding), dropout=0.3, return_sequences=True))  \n",
    "model_LSTM.add(LSTM(32, dropout=0.3, return_sequences=False))\n",
    "model_LSTM.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "model_LSTM.compile(loss='binary_crossentropy',\n",
    "                    optimizer=Adam(lr=0.001),\n",
    "                    metrics=['accuracy'])\n",
    "model_LSTM.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 2435 samples, validate on 1218 samples\n",
      "Epoch 1/4\n",
      "2435/2435 [==============================] - 15s 6ms/step - loss: 0.2994 - acc: 0.8871 - val_loss: 0.2062 - val_acc: 0.9297\n",
      "Epoch 2/4\n",
      "2435/2435 [==============================] - 7s 3ms/step - loss: 0.2274 - acc: 0.9177 - val_loss: 0.1849 - val_acc: 0.9343\n",
      "Epoch 3/4\n",
      "2435/2435 [==============================] - 7s 3ms/step - loss: 0.2157 - acc: 0.9213 - val_loss: 0.1891 - val_acc: 0.9343\n",
      "Epoch 4/4\n",
      "2435/2435 [==============================] - 8s 3ms/step - loss: 0.2116 - acc: 0.9207 - val_loss: 0.1788 - val_acc: 0.9338\n"
     ]
    }
   ],
   "source": [
    "history_LSTM = model_LSTM.fit(x_train, y_train,\n",
    "                              validation_data=(x_val, y_val),\n",
    "                              epochs=4,\n",
    "                              batch_size=64,\n",
    "                              verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The LSTM network gets a little higher accuracy on the validation set than previous DL models - even though training time is a little longer (more parameters). We think that with model fine-tuning and training on a larger dataset, LSTM could be the preferable and most performant model choice."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM with FastText embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a last try, we implemented a LSTM based on FastText embeddings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2435, 20, 100)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Pre-processing\n",
    "\n",
    "x_train = [df_dataset_train.clean_description.iloc[i][:max_words] for i in range(df_dataset_train.shape[0])]\n",
    "\n",
    "x_train_FT = []\n",
    "for i in range(len(x_train)):\n",
    "    x = np.array([np.array(embedding_matrix_FT[word]) for word in x_train[i] if word in embedding_matrix_FT.columns])\n",
    "    while x.shape[0] < max_words:\n",
    "        x = np.append(x, [np.zeros(100)], axis=0)\n",
    "    x_train_FT.append(x)\n",
    "\n",
    "x_train = np.array(x_train_FT)\n",
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1218, 20, 100)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_val = [df_dataset_test.clean_description.iloc[i][:max_words] for i in range(df_dataset_test.shape[0])]\n",
    "\n",
    "x_val_FT = []\n",
    "for i in range(len(x_val)):\n",
    "    x = np.array([np.array(embedding_matrix_FT[word]) for word in x_val[i] if word in embedding_matrix_FT.columns])\n",
    "    while x.shape[0] < max_words:\n",
    "        x = np.reshape(x, (x.shape[0],max_embedding))\n",
    "        x = np.concatenate((x, [np.zeros(max_embedding)]), axis=0)\n",
    "    x_val_FT.append(x)\n",
    "\n",
    "x_val = np.array(x_val_FT)\n",
    "x_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_9 (LSTM)                (None, 20, 64)            42240     \n",
      "_________________________________________________________________\n",
      "lstm_10 (LSTM)               (None, 32)                12416     \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 6)                 198       \n",
      "=================================================================\n",
      "Total params: 54,854\n",
      "Trainable params: 54,854\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "max_features = 1615 # nb of words in the vocabulary\n",
    "\n",
    "model_LSTM = Sequential()\n",
    "model_LSTM.add(LSTM(64, input_shape=(max_words, max_embedding), dropout=0.3, return_sequences=True))  \n",
    "model_LSTM.add(LSTM(32, dropout=0.3, return_sequences=False))\n",
    "model_LSTM.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "model_LSTM.compile(loss='binary_crossentropy',\n",
    "                    optimizer=Adam(lr=0.001),\n",
    "                    metrics=['accuracy'])\n",
    "model_LSTM.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 2435 samples, validate on 1218 samples\n",
      "Epoch 1/4\n",
      "2435/2435 [==============================] - 19s 8ms/step - loss: 0.3105 - acc: 0.8864 - val_loss: 0.2201 - val_acc: 0.9299\n",
      "Epoch 2/4\n",
      "2435/2435 [==============================] - 8s 3ms/step - loss: 0.2585 - acc: 0.9107 - val_loss: 0.2154 - val_acc: 0.9299\n",
      "Epoch 3/4\n",
      "2435/2435 [==============================] - 7s 3ms/step - loss: 0.2525 - acc: 0.9107 - val_loss: 0.2100 - val_acc: 0.9299\n",
      "Epoch 4/4\n",
      "2435/2435 [==============================] - 7s 3ms/step - loss: 0.2360 - acc: 0.9144 - val_loss: 0.1896 - val_acc: 0.9310\n"
     ]
    }
   ],
   "source": [
    "history_LSTM = model_LSTM.fit(x_train, y_train,\n",
    "                              validation_data=(x_val, y_val),\n",
    "                              epochs=4,\n",
    "                              batch_size=64,\n",
    "                              verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accuracy is very similar to LSTM with Word2Vec, which remains our preferable choice. We think that accuracy for the 2 last models (LSTM with Word2Vec, LSTM with FastText) could be significantly improved by:\n",
    "\n",
    "* training the network on a larger dataset (full scrapped data)\n",
    "* fine-tuning network hyper-parameters (model architecture, dropout rate, batch normalization, etc.)\n",
    "* adjusting the preprocessing part (number of words per review)\n",
    "* training the FastText embedding model on a larger dataset\n",
    "* fine-tuning the hyperparameters of the FastText model (embedding size, nb of epochs, etc.)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "LSI_scrapped.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
